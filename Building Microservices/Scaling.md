ჩვენ ვზრდით (scale) ჩვენს სისტემებს, ძირითადად, ორი მიზეზით. პირველი გაუმართაობასთან გასამკლავებლად: თუ გვაშინებს, რომ რამე გამოვა მწყობრიდან, მეტი რესურსი დაგვეხმარება, არა? მეორე წარმადობისთვის: მეტი დატვირთვის დამუშავება, ლატენტობის შემცირება ან ორივე ერთად. 

მოდი, გადავხედოთ ზოგად სკეილინგ ტექნიკებს და ვიფიქროთ, როგორ ვრცელდება ისინი მიკროსერვისების არქიტექტურაზე.

### Go Bigger  

ზოგიერთ ოპერაციას უბრალოდ მეტი სიმძლავრე სჭირდება. უფრო დიდი მანქანის მიღება სწრაფი CPU-ით და უკეთესი I/O-თ ხშირად აუმჯობესებს ლატენტობასა და გამტარობას, გაძლევთ საშუალებას მეტი სამუშაო ნაკლებ დროში დაამუშაოთ. 

თუმცა, სკეილინგის ეს ფორმა, რომელსაც ხშირად ვერტიკალურ სკეილინგს უწოდებენ, ძვირი ჯდება ზოგჯერ ერთი დიდი სერვერი უფრო მეტი ღირს, ვიდრე ორი პატარა, იმავე ჯამური სიმძლავრით, განსაკუთრებით მაშინ, როცა ძალიან დიდ მანქანებზე გადახვალთ.

ხანდახან თავად ჩვენი პროგრამული უზრუნველყოფა ვერ სარგებლობს დამატებითი რესურსებით. დიდი მანქანები ხშირად გვაძლევს მეტ CPU ბირთვს, მაგრამ ჩვენი კოდის საკმარისი ნაწილი არაა დაწერილი მათი გამოყენებისთვის. კიდევ ერთი პრობლემა ისაა, რომ ეს სკეილინგი ვერ აუმჯობესებს სერვერის გამძლეობას, თუ მხოლოდ ერთი ეგზემპლარი გვაქვს! მიუხედავად ამისა, ეს შეიძლება იყოს სწრაფი მოგება, განსაკუთრებით თუ იყენებთ ვირტუალიზაციის პროვაიდერს, რომელიც გაძლევთ მანქანების ზომის მარტივად შეცვლის საშუალებას.

---

### Splitting Workloads

როგორც აღწერილია მე–6 თავში, თითო ჰოსტზე ერთი მიკროსერვისის ქონა ბევრად სასურველია, ვიდრე ერთ ჰოსტზე რამდენიმე სერვისის გაშვება. თუმცა, თავიდან ბევრი ადამიანი ერთ სერვერზე რამდენიმე მიკროსერვისს ათავსებს ხარჯების შესამცირებლად ან ჰოსტის მართვის გასამარტივებლად (რაც საკამათო მიზეზია).  
მიკროსერვისები დამოუკიდებელი პროცესებია, რომლებიც ქსელით ურთიერთობენ, ამიტომ მათი საკუთარ ჰოსტებზე გადატანა მარტივია გამტარობისა და სკეილინგის გასაუმჯობესებლად. ეს ასევე ზრდის სისტემის გამძლეობას, რადგან ერთი ჰოსტის გათიშვა ნაკლებ მიკროსერვისს დააზარალებს. 

რა თქმა უნდა, შეგვიძლია საჭიროებისამებრ გავყოთ უკვე არსებული მიკროსერვისიც ნაწილებად, რათა დატვირთვას უკეთ გავუმკლავდეთ. მარტივი მაგალითი: წარმოვიდგინოთ, რომ ჩვენი accounts სერვისი უზრუნველყოფს მომხმარებელთა ფინანსური ანგარიშების შექმნასა და მართვას, ასევე უზრუნველყოფს API-ს ანგარიშების გასაკონტროლებლად და რეპორტების შესაქმნელად. ეს query შესაძლებლობა სისტემაზე მნიშვნელოვან დატვირთვას ქმნის. შეკითხვის ნაწილი არაა კრიტიკული დღის განმავლობაში შეკვეთების დამუშავებას არ სჭირდება.  
კლიენტების ფინანსური ჩანაწერების მართვა კრიტიკულია და მისი გათიშვა არ შეიძლება. ამ ორი შესაძლებლობის განცალკევებით ამცირებთ დატვირთვას კრიტიკულ accounts სერვისზე და შემოაქვთ ახალი accounts reporting სერვისი, რომელიც სპეციალურად შეკითხვებისთვისაა შექმნილი და როგორც არაკრიტიკული სისტემა, არ საჭიროებს ისეთივე გამძლეობას, როგორც ძირითადი accounts სერვისი.

---

### Spreading Your Risk  

გამძლეობისთვის სკეილინგის ერთ–ერთი გზა ისაა, რომ ყველა კვერცხი ერთ კალათაში არ ჩადოთ. მარტივი მაგალითია იმის უზრუნველყოფა, რომ ერთ ჰოსტზე რამდენიმე სერვისი არ გქონდეთ, სადაც ავარია ერთდროულად მრავალ სერვისს დაარტყამს. მაგრამ დავფიქრდეთ, რას ნიშნავს „ჰოსტი“. დღესდღეობით ჰოსტი ხშირად ვირტუალური კონცეფციაა.  

რას იზამთ, თუ თქვენი ყველა სერვისი სხვადასხვა ჰოსტზე გაქვთ, მაგრამ ეს ჰოსტები ერთი და იმავე ფიზიკურ მანქანაზე გაშვებული ვირტუალური ჰოსტებია? თუ ის მანქანა გამოვა მწყობრიდან, რამდენიმე სერვისს დაკარგავთ. ზოგი ვირტუალიზაციის პლატფორმა გაძლევთ გარანტიას, რომ ჰოსტები სხვადასხვა ფიზიკურ მანქანებზე გადანაწილდება, რათა ეს რისკი შემცირდეს.

შიდა ვირტუალიზაციის პლატფორმებზე გავრცელებულია პრაქტიკა, რომ ვირტუალური მანქანის root პარტიცია ერთიან SAN-ზე (storage area network) იყოს მიბმული. თუ ის SAN გაფუჭდება, მასზე მიბმული ყველა VM გაითიშება. SAN-ები დიდი, ძვირი და გაუმართაობის წინააღმდეგ შექმნილია. მიუხედავად ამისა, ბოლო 10 წელიწადში მინიმუმ ორჯერ მქონდა შემთხვევა, როცა ძვირადღირებული SAN გაფუჭდა და შედეგები საკმაოდ სერიოზული იყო.

კიდევ ერთი გავრცელებული გზა რისკის შესამცირებლად არის იმის უზრუნველყოფა, რომ თქვენი სერვისები ერთ რაკში არ იყონ მოთავსებული, ან რომ ისინი რამდენიმე მონაცემთა ცენტრში იყოს განაწილებული. თუ იყენებთ მომსახურების პროვაიდერს, მნიშვნელოვანია იცოდეთ, სთავაზობს თუ არა იგი SLA-ს (service-level agreement) და შესაბამისად დაგეგმოთ. თუ გჭირდებათ, რომ თქვენი სერვისები კვარტალში არ გათიშულიყო ოთხ საათზე მეტხანს, მაგრამ ჰოსტინგ პროვაიდერი გარანტიას იძლევა მხოლოდ რვა საათზე, უნდა შეცვალოთ SLA ან ალტერნატიული გამოსავალი მოძებნოთ.

მაგალითად, AWS დაყოფილია რეგიონებად, რომელთაც შეგიძლიათ ცალკეულ ღრუბლებად ჩათვალოთ. თითოეული რეგიონი თავის მხრივ იყოფა ორ ან მეტ Availability Zone-ად (AZ). AZ არის AWS-ის ეკვივალენტი მონაცემთა ცენტრისა. აუცილებელია სერვისების განაწილება რამდენიმე Availability Zone-ში, რადგან AWS არ იძლევა გარანტიას ერთ ნოდის ან მთელი Availability Zone-ის ხელმისაწვდომობაზე.  
Compute სერვისისთვის ის უზრუნველყოფს მხოლოდ 99.95%-იან ხელმისაწვდომობას რეგიონზე, ასე რომ workloads უნდა გაანაწილოთ ერთ რეგიონში რამდენიმე AZ-ზე. ზოგიერთისთვის ესეც არასაკმარისია და ისინი თავიანთ სერვისებს რამდენიმე რეგიონზეც ანაწილებენ.

უნდა აღინიშნოს, რომ მიუხედავად იმისა, რომ პროვაიდერები გაძლევენ SLA-ს გარანტიას, ისინი, როგორც წესი, ზღუდავენ საკუთარ პასუხისმგებლობას! თუ მათი მარცხი გიკარგავთ მომხმარებლებს და დიდ თანხას, შეიძლება კონტრაქტებში მოგიწიოთ ძებნა, რამე უკან რომ დაიბრუნოთ. ამიტომ გირჩევთ, კარგად გაიაზროთ მიმწოდებლის ვალდებულებების შეუსრულებლობის შედეგი და გადაწყვიტოთ, გჭირდებათ თუ არა გეგმა B (ან C). მე მქონია კლიენტები, რომლებმაც შექმნეს საგანგებო აღდგენის (disaster recovery) ჰოსტინგ-პლატფორმა სხვა პროვაიდერთან, რათა ერთ კომპანიაზე ზედმეტად დამოკიდებულები არ ყოფილიყვნენ.

---

### Load Balancing  

როდესაც გჭირდებათ სერვისის გამძლეობა, გინდათ თავიდან აიცილოთ ერთ წერტილში მარცხი (single point of failure). ტიპური მიკროსერვისისთვის, რომელიც აჩვენებს სინქრონულ HTTP endpoint-ს, ყველაზე მარტივი გზა ამის მისაღწევადაა რამდენიმე ჰოსტზე მიკროსერვისის ინსტანციის გაშვება load balancer-ის უკან, როგორც ნაჩვენებია ფიგურაში 11-4. მიკროსერვისის მომხმარებლისთვის უცნობია, ესაუბრებიან ერთ ინსტანციას თუ ასს.

![[Pasted image 20250919235053.png]]
ფიგურა 11-4. კლიენტთა სერვისის ინსტანციების რაოდენობის გასაზრდელად load balancing-ის მაგალითი

Load balancer-ები მრავალფეროვანია დიდი და ძვირი ჰარდვეარ აპლაიანსებიდან დაწყებული პროგრამულ ბალანსერებამდე, როგორიცაა mod_proxy. მათ საერთო ძირითადი შესაძლებლობები აქვთ: მათზე მისული ზარები ნაწილდება ერთ ან მეტ ინსტანციაზე გარკვეული ალგორითმით, ჯანმრთელობის დაკარგვისას ინსტანცია ამოიშლება და, იმედია, ჯანმრთელობის აღდგენისას ისევ დაემატება.

ზოგი load balancer სასარგებლო ფუნქციებსაც გვთავაზობს. ერთ-ერთი გავრცელებულია SSL termination  შემომავალი HTTPS კავშირები load balancer-ზე გარდაიქმნება HTTP კავშირებად ინსტანციაზე მისვლისას. ისტორიულად, SSL-ის მართვის ზედნადები იმდენად მნიშვნელოვანი იყო, რომ load balancer-ის მეშვეობით ამის გაკეთება საკმაოდ სასარგებლო იყო. დღეს ეს უფრო ინდივიდუალური ჰოსტების გამარტივებას ემსახურება.  

თუმცა HTTPS-ის გამოყენების მიზანია მოთხოვნების დაცვა man-in-the-middle შეტევისგან, როგორც მე–9 თავში განვიხილეთ, ასე რომ SSL termination გარკვეულ რისკს ქმნის. ერთ-ერთი შემამსუბუქებელი გზა ისაა, რომ მიკროსერვისის ყველა ინსტანცია მოთავსდეს ერთ VLAN-ში (virtual local area network), როგორც ნაჩვენებია ფიგურაში 11-5. VLAN იზოლირებულია ისე, რომ გარე მოთხოვნები მასზე მოდის მხოლოდ როუტერის მეშვეობით, რომელიც ამ შემთხვევაში არის SSL-terminating load balancer. გარედან მიკროსერვისთან კომუნიკაცია ხდება HTTPS-ით, ხოლო შიგნით ყველაფერი HTTP-ით.

![[Pasted image 20250919235112.png]]
ფიგურა 11-5. HTTPS termination-ის გამოყენება load balancer-ზე VLAN-თან ერთად უსაფრთხოების გასაუმჯობესებლად

AWS გთავაზობთ HTTPS-terminating load balancer-ებს ELB-ების (elastic load balancer) სახით და შეგიძლიათ გამოიყენოთ მისი security group-ები ან virtual private cloud-ები (VPC) VLAN-ის განსახორციელებლად. სხვა შემთხვევაში, mod_proxy-ს მსგავსი პროგრამული უზრუნველყოფაც შეიძლება იყოს პროგრამული load balancer.  

ბევრ ორგანიზაციას აქვს hardware load balancer-ები, რომლებიც ავტომატიზაციას ართულებს. ასეთ შემთხვევებში მე ხშირად ვამტკიცებ, რომ hardware load balancer-ების უკან დავაყენოთ software load balancer-ები, რათა გუნდებს კონფიგურაციის მოქნილად შეცვლის თავისუფლება ჰქონდეთ. თუმცა ყურადღება მიაქციეთ, რომ hardware load balancer-ებიც ხშირად თავად არიან single point of failure!  

რომელი მიდგომაც არ უნდა აირჩიოთ, load balancer-ის კონფიგურაციას ისე მოეკიდეთ, როგორც თქვენი სერვისის კონფიგურაციას: შეინახეთ ვერსიის კონტროლში და უზრუნველყავით მისი ავტომატურად გამოყენების შესაძლებლობა.

Load balancer-ები გვაძლევენ საშუალებას მიკროსერვისის მეტი ინსტანცია დავამატოთ ისე, რომ ეს მომხმარებლისთვის შეუმჩნეველი იყოს. ეს ზრდის დატვირთვის დამუშავების შესაძლებლობას და ამცირებს ერთ ჰოსტზე მარცხის გავლენას. თუმცა, თქვენი მიკროსერვისების უმეტესობას ექნება რაიმე მუდმივი მონაცემთა საცავი ალბათ მონაცემთა ბაზა, რომელიც სხვა მანქანაზეა.  
თუ მიკროსერვისის მრავალი ინსტანცია გვაქვს სხვადასხვა მანქანაზე, მაგრამ მონაცემთა ბაზა მხოლოდ ერთ ჰოსტზეა, ჩვენი მონაცემთა ბაზა მაინც ერთ წერტილში მარცხის წყაროა. ამასთან გასამკლავებლად პატერნებზე მალე ვისაუბრებთ.

---
### **ვორქერებზე დაფუძნებული სისტემები**  

ჩატვირთვის ბალანსირება არ არის ერთადერთი გზა, რომლითაც თქვენი სერვისის მრავალი ინსტანცია შეძლებს ტვირთის გაზიარებასა და მტვრევადობის შემცირებას. 

ოპერაციების ბუნების მიხედვით, ვორქერებზე (worker) დაფუძნებული სისტემა შეიძლება არანაკლებ ეფექტიანი იყოს. ამ მოდელში ინსტანციების კოლექცია მუშაობს ერთობლივ სამუშაო რიგზე (backlog). ეს შეიძლება იყოს რამდენიმე Hadoop პროცესის გაშვება, ან შესაძლოა რამდენიმე მსმენელი ერთობლივ სამუშაო რიგში. ასეთი ტიპის ოპერაციები შესანიშნავად ერგება ბაჩ-ამოცანებს ან ასინქრონულ სამუშაოებს წარმოიდგინეთ ამოცანები, როგორიცაა გამოსახულების მინიატურების დამუშავება, ელფოსტის გაგზავნა ან რეპორტების გენერაცია.

ეს მოდელი ასევე კარგად მუშაობს პიკური დატვირთვისას, როცა შეგიძლიათ მოთხოვნის შესაბამისად დააწყვილოთ დამატებითი ინსტანციები. სანამ სამუშაო რიგი თვითონაა გამძლე, ეს მოდელი შეიძლება გამოყენებულ იქნეს როგორც გამტარუნარიანობის გასაზრდელად, ასევე გამძლეობის გასაუმჯობესებლად ვორქერებზე ჩავარდნის (ან არარსებობის) გავლენა მარტივად სამართავია. სამუშაო შეიძლება უფრო დიდხანს გაგრძელდეს, მაგრამ არაფერი დაიკარგება.

მე პირადად მინახავს, რომ ეს კარგად მუშაობს ორგანიზაციებში, სადაც დღის გარკვეულ მონაკვეთებში ბევრი გამოუყენებელი გამოთვლითი რესურსია. მაგალითად, ღამით შეიძლება აღარ გჭირდებოდეთ იმდენი მანქანა თქვენი ecommerce სისტემის გასაშვებად, ამიტომ შეგიძლიათ დროებით გამოიყენოთ ისინი ანგარიშგების სამუშაოს ვორქერებად.

ვორქერებზე დაფუძნებულ სისტემებში, მიუხედავად იმისა, რომ თავად ვორქერები არ უნდა იყვნენ ძალიან საიმედო, ის სისტემა, რომელიც სამუშაოს ინახავს, უნდა იყოს გამძლე. ამის მართვა შეგიძლიათ, მაგალითად, მუდმივი მესიჯ-ბროკერის გაშვებით ან ისეთი სისტემის გამოყენებით, როგორიცაა Zookeeper. უპირატესობა ისაა, რომ თუ ამ მიზნისთვის უკვე არსებულ პროგრამულ უზრუნველყოფას გამოვიყენებთ, ვიღაცამ უკვე გააკეთა სამუშაოს დიდი ნაწილი. თუმცა მაინც გვჭირდება იმის ცოდნა, თუ როგორ მოვაწყოთ და შევინარჩუნოთ ეს სისტემები გამძლე ფორმით.

---

### **თავიდან დაწყება**  

არქიტექტურა, რომელიც დაგეხმარებათ დასაწყისში, შესაძლოა აღარ გამოდგეს მაშინ, როცა თქვენს სისტემას სრულიად განსხვავებული მოცულობის დატვირთვის მართვა მოუწევს. როგორც ჯეფ დინმა თქვა თავის პრეზენტაციაში _“Challenges in Building Large-Scale Information Retrieval Systems”_ (WSDM 2009 კონფერენცია): _„დააპროექტეთ ~10× ზრდისთვის, მაგრამ დაგეგმეთ ხელახალი გადაწერა ~100×-მდე.“_ გარკვეულ მომენტებში, თქვენ უნდა გააკეთოთ საკმაოდ რადიკალური რამ შემდეგი დონის ზრდის მხარდასაჭერად.

გახსოვდეთ Gilt-ის ისტორია, რომელსაც მე-6 თავში შევეხეთ. მარტივმა მონოლითურმა Rails აპლიკაციამ Gilt-ს ორი წლის განმავლობაში კარგად უმუშავა. ბიზნესის წარმატებამ მეტი მომხმარებელი და შესაბამისად მეტი დატვირთვა მოიტანა. გარკვეულ კრიტიკულ წერტილზე კომპანიამ აპლიკაციის არქიტექტურა ხელახლა უნდა დააპროექტოს, რათა გაუმკლავდეს გაზრდილ დატვირთვას.

ხელახალი დიზაინი შეიძლება ნიშნავდეს არსებული მონოლითის დაყოფას, როგორც ეს Gilt-ის შემთხვევაში მოხდა. ან შეიძლება ნიშნავდეს ახალი მონაცემთა საცავების შერჩევას, რომლებიც უკეთ გაუმკლავდებიან დატვირთვას — რაზეც მალე ვისაუბრებთ. ასევე შეიძლება ნიშნავდეს ახალი ტექნიკების დანერგვას, მაგალითად სინქრონული მოთხოვნა/პასუხიდან მოვლენებზე დაფუძნებულ სისტემებზე გადასვლას, ახალი განთავსების პლატფორმების გამოყენებას, მთელი ტექნოლოგიური სტეკის შეცვლას ან ამ ყველაფერს შორის.

არის საფრთხე, რომ ადამიანები მასშტაბირების გარკვეული ზღვრების მიღწევისას საჭირო რეჰარქიტექტურას მიზეზად გამოიყენებენ თავიდანვე უზარმაზარ მასშტაბზე აშენებისთვის. ეს შეიძლება კატასტროფული იყოს. ახალი პროექტის დასაწყისში ჩვენ ხშირად არ ვიცით ზუსტად რა გვინდა ავაშენოთ და არც ის, იქნება თუ არა ის წარმატებული. გვჭირდება სწრაფი ექსპერიმენტების ჩატარების შესაძლებლობა და იმის გაგება, რა შესაძლებლობები გვჭირდება. თუ massive scale-ს თავიდანვე ვცდილობთ, დიდ მოცულობის სამუშაოს წინასწარ ჩავდებთ დატვირთვისთვის, რომელიც შესაძლოა საერთოდ არ მოვიდეს, და ამით ვაცილებთ ძალისხმევას უფრო მნიშვნელოვან საქმიანობას, მაგალითად იმის გაგებას, სურს კი ვინმეს რეალურად გამოიყენოს ჩვენი პროდუქტი. ერიკ რიზი ჰყვება ისტორიას, თუ როგორ გაატარა ექვსი თვე ისეთი პროდუქტის აშენებაში, რომელიც არც ერთმა მომხმარებელმა არ ჩამოტვირთა. ის ფიქრობდა, რომ შეეძლო უბრალოდ ვებგვერდზე დაედო ბმული, რომელიც 404 შეცდომას აჩვენებდა და ენახა მოთხოვნა თუ იყო, ექვსი თვე სანაპიროზე გაეტარებინა და იგივე ესწავლა!

სისტემების მასშტაბირებისთვის შეცვლის საჭიროება მარცხის ნიშანი არ არის  ეს წარმატების ნიშანია.