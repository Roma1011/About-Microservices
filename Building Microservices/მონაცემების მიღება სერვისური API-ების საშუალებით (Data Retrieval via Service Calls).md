ამ მოდელის მრავალი ვარიანტია, მაგრამ ყველა ეფუძნება მონაცემების მიღებას წყარო სისტემებიდან API-ზედმეტებით. ძალიან მარტივი ანგარიშგების სისტემის შემთხვევაში, მაგალითად **დაშბორდი**, რომელიც შეიძლება მხოლოდ ბოლო 15 წუთში განხორციელებული შეკვეთების რაოდენობას აჩვენებდეს, ეს სრულიად მისაღებია. თუმცა, თუ საჭიროა ორი ან მეტი სისტემის მონაცემების გაერთიანება, საჭიროა მრავალჯერადი API გამოძახება მონაცემების ასაწყობად.

ამ მიდგომა სწრაფად მარცხდება, როცა საქმე უფრო დიდ მოცულობებს მოითხოვს. წარმოიდგინეთ შემთხვევა, სადაც გვინდა გამოვყოთ **მომხმარებელთა შეძენითი ქცევა** ჩვენს მუსიკის მაღაზიაში უკანასკნელი 24 თვის განმავლობაში, ვაკვირდებით მომხმარებელთა ქცევის სხვადასხვა ტენდენციას და როგორ აისახა ეს შემოსავალზე. ამისთვის საჭიროა დიდი მოცულობის მონაცემების მიღება, სულ მცირე, **კლიენტის** და **ფინანსთა სისტემებიდან**.

მონაცემების ადგილობრივი კოპირების შენახვა ანგარიშგების სისტემაში სახიფათოა, რადგან შეიძლება არ ვიცოდეთ, შეიცვალა თუ არა მონაცემები (გადმოცემული ისტორიული მონაცემიც შეიძლება შეცვალონ), ამიტომ ზუსტი ანგარიშის შესაქმნელად საჭიროა ბოლო ორი წლის **ფინანსებისა და კლიენტის ჩანაწერები**. საშუალო რაოდენობის კლიენტების შემთხვევაშიც კი ეს ოპერაცია შეიძლება ძალიან ნელი გახდეს.

ანგარიშგების სისტემები ხშირად ემყარებიან მესამე მხარის ინსტრუმენტებს, რომლებიც მონაცემებს გარკვეული გზით იღებენ, ამიტომ SQL ინტერფეისის მიწოდება ყველაზე სწრაფი გზა იქნება, რათა ანგარიშგების ინსტრუმენტების ჯაჭვი მარტივად ინტეგრირდეს. ჩვენ მაინც შეგვიძლია პერიოდულად მონაცემები SQL მონაცემთა ბაზაში გადაიტანოთ, მაგრამ ეს მაინც გარკვეულ გამოწვევებს წარმოქმნის.

---

**მნიშვნელოვანი გამოწვევები**:  
API-ები, რომლებიც სხვადასხვა მიკროსერვისების მიერაა წარმოდგენილი, შესაძლოა ვერ იყოს გათვლილი ანგარიშგებისთვის. მაგალითად, კლიენტის სერვისი შეიძლება საშუალებას გაძლევდეთ მოძებნოთ კლიენტი ID-ს მიხედვით, ან სხვადასხვა ფილდებით მოძებნოთ კლიენტი, მაგრამ აუცილებლად არ გაუწვდეს API-ს, რომელიც ყველა კლიენტს აბრუნებს. შედეგად შეიძლება უამრავი გამოძახება განხორციელდეს ყველა მონაცემის მისაღებად — მაგალითად, ყველა კლიენტის სიაში უნდა გავიაროთ ცალ-ცალკე, თითოეული გამოსაძახებლად. ეს არა მხოლოდ არაეფექტურია ანგარიშგებისთვის, არამედ სერვისსაც დატვირთვას ახდენს.

მონაცემების მიღების დაჩქარება შესაძლებელია **cache headers-ის** დამატებით სერვისის რესურსებზე და მონაცემების **reverse proxy**-ში კეშირებით. თუმცა, ანგარიშგების ბუნება ხშირად მოითხოვს **long tail** მონაცემების წვდომას. ეს ნიშნავს, რომ შესაძლოა რესურსები მოთხოვნათ, რომელსაც ადრე არავის სთხოვია (ან საკმაოდ დიდი ხნის განმავლობაში), რაც იწვევს შედარებით ძვირადღირებულ cache miss-ს.

---

**Batch API-ების გამოყენება**  
ეს შეიძლება გამოიწვიოს **batch API-ების** შექმნით ანგარიშგების გასამარტივებლად. მაგალითად, კლიენტის სერვისი შეიძლება მისცეს შესაძლებლობა, რომ მიაწოდოთ კლიენტთა ID-ების სია, რათა მიიღოთ ისინი ჯგუფებად, ან შესაძლოა წარმოადგინოს ინტერფეისი, რომელიც საშუალებას გაძლევთ გვერდზე გადახედოთ ყველა კლიენტს. უფრო ექსტრემალური ვერსიაა **batch request-ის რესურსად მოდელირება**.

მაგალითად, კლიენტის სერვისი შეიძლება წარმოადგინოს **BatchCustomerExport** რესურსის endpoint. გამოსაკითხი სისტემა გააგზავნის **POST BatchRequest**, შესაძლოა მიუთითოს ლოკაცია, სადაც ფაილი შენახული იქნება ყველა მონაცემისთვის. კლიენტის სერვისი დაბრუნებს **HTTP 202**, რაც მიუთითებს, რომ მოთხოვნა მიღებულია, მაგრამ ჯერ დამუშავებული არ არის. გამოძახება სისტემა პერიოდულად შეამოწმებს რესურსს, სანამ **201 Created** სტატუსს არ მიიღებს, რაც მიუთითებს, რომ მოთხოვნა შესრულებულია, შემდეგ კი მონაცემებს გადმოწერს. ეს საშუალებას იძლევა დიდი მონაცემთა ფაილები გაიგზავნოს HTTP-ს გარეშე, მაგალითად CSV ფაილი შეინახოთ საერთო ლოკაციაში.

---

მე დავინახე, რომ ეს მიდგომა წარმატებით გამოიყენებოდა **batch insertion-ისთვის**, მაგრამ ანგარიშგების სისტემებისთვის ნაკლებად მომწონს, რადგან მგონია, რომ არსებობს სხვა, შედარებით მარტივი გადაწყვეტილებები, რომლებიც უფრო ეფექტურად მასშტაბირდება ტრადიციული ანგარიშგების საჭიროებების შემთხვევაში.